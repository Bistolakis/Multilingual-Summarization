# Multilingual-Summarization

Natural language processing has grown a lot in the modern age of Big Data. One of the major
goals of the scientific community is the creation of abstract summaries from machines.
This goal also includes the creation of models that will be capable of producing summaries
in multiple languages. This thesis uses multilingual models such as mT5 to generate abstract
summaries using related languages during fine-tuning. In particular, it seems that models
like these can generalize their ability to summarize in many languages, even if they have few
samples in fine-tuning. The use of related languages seems to help individual languages with
improved quality in summaries compared to models witch are trained with one language

data. Also, this method seems to be a good alternative when there is not enough amount
of data from a language, since the ability to summarize seems to be transferred more easily
from one language to another when these are related. Overall research shows that using
related languages to train multilingual models is more efficient, because in less training time
the summaries are improved compared to models which where trained in each language
separately. As a conclusion, it is preferable by the scientific community to use multilingual
models since they are more efficient and useful as well.
